\documentclass{article}
\usepackage{mathtools}

\begin{document}
\title{Deconvolution of Two Dimensional Matrices}
\author{William McInroy}
\date{July 2014}
\maketitle

\begin{abstract}
Convolution in image processing by nature sub-samples the data in a sometimes unwanted manner. Sometimes it is needed to regenerate this previous data, and that is when deconvolution is introduced. This process compares the outputs to the given convolution kernal and attempts to reconstruct the lost data from the convolved data.
\end{abstract}

\section{Process}

You are given two arguments. $k_{nm}$, the kernal, and $x_{ij}$, the convolved matrix. For each cell in $x_{ij}$, you create a new matrix of the size $n\times m$ (the same size as the kernal). The values of this matrix are determined by the line of symmetry in the matrix. The line of symmetry in this context is not the same as regular mathematics, as it is where the values directly across the line of zeros in the kernal are the negative of the previous cell. An example of a kernal with vertical symmetry

$ \left( \begin{array}{ccc}
a & b & c \\
0 & 0 & 0 \\
-a & -b & -c \end{array} \right)$

The values of this matrix are then mapped to a vector only if the value of $k_{nm} \neq 0$ . With an example kernal that looks like:

$\left( \begin{array}{ccc}
-1 & -1 & -1 \\
0 & 0 & 0 \\
1 & 1 & 1 \end{array} \right)$

Which as you may notice is a horizontal line detector, you produce a vector of:

$\langle k_n \rangle = \langle -1, -1, -1, 1, 1, 1 \rangle$

 With an example value at $x_{00}$ of $4$. Since $x_{00}$ is positive, the average values of the kernal in the positive half of the symmetry must be larger than the values convolved by the negative portion. The average amount larger that each cell is given by


${x_{00}} \over {n/2}$

$n$ is the length of the vector. $n$ is divided by two because the elements of the vector are symmetrical absolute values. This creates a new matrix that is

$\left( \begin{array}{ccc}
1 & 1 & 1 \\
0 & 0 & 0 \\
\frac{4}{3} & \frac{4}{3} & \frac{4}{3} \end{array} \right)$

The same vector casting is applied here as to the kernal, and the following is produced.

$\langle m_n \rangle = \langle 1, 1, 1, \frac{4}{3}, \frac{4}{3}, \frac{4}{3} \rangle$

The equation of convolution can now be interpreted as


$x_{ij}=\langle k_n \rangle \cdot \langle m_n \rangle \times C$

where $C$ is an unknown constant. Solving for $C$ will enable us to deconvolve $x_{ij}$.

Solving for $C$ in this case is


$C = \frac{x_{ij}}{\langle k_n \rangle \cdot \langle m_n \rangle} = 4$

The fact that $C = 4 = x_{00}$ is a coincidence. We can now multiply that constant by $\langle m_n \rangle$ which will produce


$\langle y_n \rangle = \langle m_n \rangle \times C = \langle 4, 4, 4, 5 + \frac{1}{3}, 5 + \frac{1}{3}, 5 + \frac{1}{3} \rangle$

Now to prove that $\langle y_n \rangle$ is the deconvolved vector, the dot product shows that


$\langle y_n \rangle \cdot \langle k_n \rangle = x_{00}$

The $\langle y_n \rangle$ vector can be expanded to a matrix, substituting the non-zero values for values in the matrix. The elements of the matrix can randomly be increased and decreased, as long as they stay in proportion. 

\section{Known Issues}
The main issue with this solution, is that it is made for an ill-posed problem. There are infinite different correct solutions. For example, the problem proposed in the section before, another correct answer would be:

$\langle y_n \rangle = \langle m_n \rangle \times C = \langle 7, 2, 3, 5 + \frac{1}{3}, 6 + \frac{1}{3}, 4 + \frac{1}{3} \rangle$



\end{document}